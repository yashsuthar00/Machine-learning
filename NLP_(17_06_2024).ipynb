{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcArOSP3TXKFhSM203eOBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashsuthar00/Machine-learning/blob/main/NLP_(17_06_2024).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL63O17BeO1R",
        "outputId": "55600fad-c428-4af3-ec75-fa9c9c4d319f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done all\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"done all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenization :---Breaking a sentence into individual words or pieces, like cutting a cake into slices.\n",
        "\n",
        "Stemming and Lemmatization :--Changing words to their base form, like turning \"running\" into \"run\" so we know they mean the same thing.\n",
        "\n",
        "\n",
        "Stop Words :--Removing common words like \"and\" and \"the\" that don't add much meaning to a sentence.\n",
        "\n",
        "Vocabulary and Matching:--Keeping a list of all the words we know and finding words that match in different sentences.\n",
        "\n",
        "Parts of Speech Tagging:--Labelling words as nouns, verbs, or adjectives, like sorting toys into different bins.\n",
        "\n",
        "Named Entity Recognition:--Finding and naming important words in a sentence, like recognizing \"Harry Potter\" as a person.\n"
      ],
      "metadata": {
        "id": "lggSrCulhQSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "string = \"Hey Simran I am with you for the entire life !\"\n",
        "print(string)\n",
        "\n",
        "data = nlp(string)\n",
        "\n",
        "#print tokenized words\n",
        "for token in data:\n",
        "    print(token.text, end=\" | \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpVFStO2hakf",
        "outputId": "68fa1092-7369-420f-a20d-16d23d2432ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey Simran I am with you for the entire life !\n",
            "Hey | Simran | I | am | with | you | for | the | entire | life | ! | "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "data = nlp(u\"Simran I am here to help! call me, msg me or email me\")\n",
        "for token in data:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2o4Bp0siYrB",
        "outputId": "1440d776-422e-4a06-9cc8-30d76cbb4a63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simran\n",
            "I\n",
            "am\n",
            "here\n",
            "to\n",
            "help\n",
            "!\n",
            "call\n",
            "me\n",
            ",\n",
            "msg\n",
            "me\n",
            "or\n",
            "email\n",
            "me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "data=nlp(u\"T have total $10.20 and I am going to buy $2.30 choco\")\n",
        "for token in data:\n",
        "  print(token)\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGiS-nyrk-Lq",
        "outputId": "db105a22-67d3-4381-856c-10ea91e8e13e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T\n",
            "have\n",
            "total\n",
            "$\n",
            "10.20\n",
            "and\n",
            "I\n",
            "am\n",
            "going\n",
            "to\n",
            "buy\n",
            "$\n",
            "2.30\n",
            "choco\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "data=nlp(u\"T have total $10.20 and I am going to buy $2.30 choco\")\n",
        "print(data[2])\n",
        "print(data[2:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkaFbHkflMgx",
        "outputId": "f3de12d6-ca40-4be0-c2df-649ebaef55d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total\n",
            "total $10.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "data=nlp(u\"Apple to built a India factory for $6 million\")\n",
        "print(\"Detailed Token Label Information\")\n",
        "\n",
        "for token in data:\n",
        "  print(token.text + \" | \" + token.ent_type_ + \" | \" + str(spacy.explain(token.ent_type_)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_eMR8ual6O4",
        "outputId": "7f981fe1-7171-4a3f-d6e3-0b128fbd877f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detailed Token Label Information\n",
            "Apple | ORG | Companies, agencies, institutions, etc.\n",
            "to |  | None\n",
            "built |  | None\n",
            "a |  | None\n",
            "India | GPE | Countries, cities, states\n",
            "factory |  | None\n",
            "for |  | None\n",
            "$ | MONEY | Monetary values, including unit\n",
            "6 | MONEY | Monetary values, including unit\n",
            "million | MONEY | Monetary values, including unit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "data=nlp(u\"Apple to built a India factory for $6 million\")\n",
        "print(\"Detailed Token Label Information\")\n",
        "\n",
        "for token in data.ents:\n",
        "  print(token.text + \" | \" + token.label_ + \" | \" + str(spacy.explain(token.label_)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx4iGyPcnLgU",
        "outputId": "2cab04bb-c180-4dec-a49d-b5031509273a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detailed Token Label Information\n",
            "Apple | ORG | Companies, agencies, institutions, etc.\n",
            "India | GPE | Countries, cities, states\n",
            "$6 million | MONEY | Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "#Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#List of words to be stemmed\n",
        "\n",
        "words = [\"running\", \"jumps\", \"jumped\", \"running\", \"files\", \"happily\", \"better\", \"betterment\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word + \" == \" + stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScNY16X-n9hh",
        "outputId": "016e998c-b251-4893-c1e1-d993ef0f6f3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running == run\n",
            "jumps == jump\n",
            "jumped == jump\n",
            "running == run\n",
            "files == file\n",
            "happily == happili\n",
            "better == better\n",
            "betterment == better\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Create a SnowballStemmer object for English\n",
        "snowball_stem = SnowballStemmer(\"english\")\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"happiness\", \"running\", \"jumps\", \"easily\", \"fairly\"]\n",
        "\n",
        "for words in words:\n",
        "  print(words+ \" -- \" + snowball_stem.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7duW6YtpEbd",
        "outputId": "15a188a1-57cd-40a7-a4ce-89803e85a494"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -- run\n",
            "happiness -- happi\n",
            "running -- run\n",
            "jumps -- jump\n",
            "easily -- easili\n",
            "fairly -- fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Create a snowballStemmer object\n",
        "snowball_stem = SnowballStemmer(\"english\")\n",
        "\n",
        "words = [\"consolingly\"]\n",
        "\n",
        "\n",
        "for word in words:\n",
        "  print(word + \" -- \" + porter_stemmer.stem(word))\n",
        "\n",
        "for word in words:\n",
        "  print(word + \" -- \" + snowball_stem.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z3ceMtuqODv",
        "outputId": "6b20b64b-e7f0-4077-97a6-01504842c4ed"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "consolingly -- consolingli\n",
            "consolingly -- consol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc1 = nlp(\"The striped bats are hanging on their feet for best\")\n",
        "for token in doc1:\n",
        "  print(token.text, '\\t', token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ID_rrH8rn_A",
        "outputId": "348f353d-c8c2-480d-bd00-7bfd1f72091a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \t the\n",
            "striped \t stripe\n",
            "bats \t bat\n",
            "are \t be\n",
            "hanging \t hang\n",
            "on \t on\n",
            "their \t their\n",
            "feet \t foot\n",
            "for \t for\n",
            "best \t good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "s1 = \"The striped bats are hanging on their feet for best\"\n",
        "for word5 in s1.split():\n",
        "  print(word + ' ------ ' + p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGq9iPOhsWZ9",
        "outputId": "7aec6356-6ce0-4080-ea5b-ab7dbf7fb2e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ------ the\n",
            "striped ------ stripe\n",
            "bats ------ bat\n",
            "are ------ are\n",
            "hanging ------ hang\n",
            "on ------ on\n",
            "their ------ their\n",
            "feet ------ feet\n",
            "for ------ for\n",
            "best ------ best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(nlp.Defaults.stop_words)\n",
        "\n",
        "print(len(nlp.Defaults.stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3wubkrwuUj9",
        "outputId": "bfa035d4-5141-411c-f948-5662f3d273e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'front', 'therein', 'now', 'always', 'who', 'until', '’ve', 'already', 'nowhere', 'alone', 'ourselves', 'sixty', 'at', 'an', 'neither', 'further', 'although', 'done', 'on', 'serious', 'perhaps', 'none', 'between', 'again', 'our', 'so', 'hereupon', 'rather', 'before', 'less', 'nine', 're', 'those', 'never', 'enough', '’m', 'several', 'their', 'here', 'anyway', 'often', 'another', 'nor', 'eight', 'this', 'by', 'these', 'may', 'indeed', \"'m\", 'without', 'while', '’s', 'with', 'noone', 'across', 'others', 'he', 'its', 'somewhere', 'anywhere', 'could', 'the', 'herself', 'besides', 'due', 'them', 'herein', 'nothing', 'my', 'about', 'are', 'themselves', 'will', 'hundred', 'thereafter', 'should', 'were', 'when', 'she', 'and', 'anyone', 'beyond', 'over', 'among', 'any', 'why', 'give', 'but', '‘re', 'does', 'eleven', \"n't\", 'together', 'had', 'well', 'fifteen', 'more', 'such', 'would', \"'d\", 'beside', 'down', '‘ll', 'thru', 'someone', 'through', 'yet', 'name', 'amount', 'there', 'because', '‘s', 'whither', 'anyhow', 'must', 'no', 'towards', 'toward', 'what', 'him', 'became', 'other', 'empty', 'n‘t', 'ours', 'up', 'than', 'onto', 'back', 'various', 'former', 'am', 'might', 'it', 'off', 'however', 'meanwhile', 'next', 'behind', 'how', 'still', 'top', '’re', 'move', 'much', 'myself', 'a', 'do', 'call', 'five', 'i', 'thus', 'except', 'become', 'since', \"'s\", 'thereby', 'was', 'seem', 'whether', 'everyone', 'moreover', 'below', 'sometimes', 'using', 'thence', 'seems', 'that', 'whom', 'please', 'mostly', '‘ve', 'where', 'all', 'afterwards', 'cannot', 'we', 'hers', 'make', 'otherwise', 'her', '‘m', 'just', 'during', 'or', 'somehow', '’ll', 'been', 'to', 'most', 'can', 'after', 'in', 'quite', 'every', 'wherever', 'then', 'very', 'yours', 'full', 'nevertheless', 'once', 'along', 'via', 'both', 'throughout', 'used', 'amongst', 'take', 'latterly', 'upon', 'for', 'else', \"'ll\", 'made', 'fifty', 'third', 'beforehand', 'twenty', 'whenever', 'almost', 'as', 'elsewhere', 'out', 'get', 'everywhere', 'ever', 'go', 'into', 'hence', 'put', 'therefore', 'nobody', 'whose', 'least', 'too', 'us', 'ca', 'also', \"'re\", 'from', 'above', \"'ve\", 'per', 'only', 'under', 'whoever', 'being', 'whereas', 'which', 'anything', 'whereby', 'see', 'has', 'say', 'wherein', 'keep', 'me', 'whereupon', 'show', 'four', 'seemed', 'each', 'regarding', '’d', 'did', 'whatever', 'you', 'sometime', 'himself', 'latter', 'formerly', '‘d', 'something', 'itself', 'around', 'namely', 'whereafter', 'though', 'doing', 'yourselves', 'hereby', 'becomes', 'own', 'same', 'thereupon', 'side', 'part', 'some', 'they', 'not', 'against', 'two', 'whence', 'hereafter', 'six', 'really', 'of', 'if', 'seeming', 'mine', 'yourself', 'one', 'your', 'even', 'last', 'first', 'be', 'many', 'few', 'forty', 'his', 'becoming', 'whole', 'unless', 'ten', 'three', 'is', 'twelve', 'have', 'within', 'n’t', 'everything', 'bottom', 'either'}\n",
            "326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}